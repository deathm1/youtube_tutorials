{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql.functions import array_distinct\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/k0shur/anaconda3/envs/data_science_and_engineering/lib/python3.10/site-packages/pyspark'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+----------+-------------------+\n",
      "|  a|  b|   c|         d|                  e|\n",
      "+---+---+----+----------+-------------------+\n",
      "|  1|4.0|GFG1|2000-08-01|2000-08-01 12:00:00|\n",
      "|  2|8.0|GFG2|2000-06-02|2000-06-02 12:00:00|\n",
      "|  4|5.0|GFG3|2000-05-03|2000-05-03 12:00:00|\n",
      "+---+---+----+----------+-------------------+\n",
      "\n",
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|               email|\n",
      "+----------+---------+--------------------+\n",
      "|     John1|      Doe|john1doe@johndoe.com|\n",
      "|     John2|      Doe|john2doe@johndoe.com|\n",
      "|     John3|      Doe|john3doe@johndoe.com|\n",
      "|     John4|      Doe|john4doe@johndoe.com|\n",
      "|     John5|      Doe|john5doe@johndoe.com|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_row_list = [\n",
    "    Row(a=1, b=4., c='GFG1', d=date(2000, 8, 1),\n",
    "        e=datetime(2000, 8, 1, 12, 0)),\n",
    "   \n",
    "    Row(a=2, b=8., c='GFG2', d=date(2000, 6, 2),\n",
    "        e=datetime(2000, 6, 2, 12, 0)),\n",
    "   \n",
    "    Row(a=4, b=5., c='GFG3', d=date(2000, 5, 3),\n",
    "        e=datetime(2000, 5, 3, 12, 0))\n",
    "]\n",
    "\n",
    "spark.createDataFrame(my_row_list).show()\n",
    "\n",
    "my_tuple_list = [\n",
    "    (\"John1\", \"Doe\", \"john1doe@johndoe.com\"),\n",
    "    (\"John2\", \"Doe\", \"john2doe@johndoe.com\"),\n",
    "    (\"John3\", \"Doe\", \"john3doe@johndoe.com\"),\n",
    "    (\"John4\", \"Doe\", \"john4doe@johndoe.com\"),\n",
    "    (\"John5\", \"Doe\", \"john5doe@johndoe.com\")\n",
    "]\n",
    "my_tuple_column_list = [\"first_name\", \"last_name\", \"email\"]\n",
    "\n",
    "my_dataframe_tuple_list = spark.createDataFrame(my_tuple_list, my_tuple_column_list).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect python with SQL using odbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 18 for SQL Server};SERVER=10.0.2.5;DATABASE=TEST;UID=remote_connector;PWD=admin123;TrustServerCertificate=yes;')\n",
    "\n",
    "\n",
    "# Create a cursor from the connection\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "cursor.execute(\"select * from test_table;\")\n",
    "\n",
    "print(len(cursor.fetchall()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|           test_col1|       test_col2|\n",
      "+--------------------+----------------+\n",
      "|              asdASD|          ASDasd|\n",
      "|            sadfasdf|        asdfasdf|\n",
      "|            asdfasdf|        asdfasdf|\n",
      "|            asdfasdf|        asdfasdf|\n",
      "|                asdf|            asdf|\n",
      "|                asdf|        asdfasdf|\n",
      "|                asdf|            asdf|\n",
      "|                asdf|            asdf|\n",
      "|          aqw4352345|            2345|\n",
      "|                2345|            2345|\n",
      "|                  23|          452345|\n",
      "|234523452345234ou...| uioyhwioueyroiu|\n",
      "|            sdfgsdfg|            yoiu|\n",
      "|               yoiuy|            oiuy|\n",
      "|                 oiu|            yoiu|\n",
      "|                yoiu|               o|\n",
      "|           dsafgsdfg|        sdfgsdfg|\n",
      "|      asdasdashdjikh|hkjahsjkdhjkashd|\n",
      "|       kashdkjashdkj|hkjhkjhakjhsdjka|\n",
      "|      akjshdkjashdjk|   akjshdkjahdjk|\n",
      "|         aksjdhkajhd| kjahsdkjashsjkd|\n",
      "|           kajhskdja|      kjhkjasdas|\n",
      "+--------------------+----------------+\n",
      "\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "server_name = \"jdbc:sqlserver://10.0.2.5:1433\"\n",
    "database_name = \"TEST\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"test_table\"\n",
    "username = \"remote_connector\"\n",
    "password = \"admin123\"\n",
    "\n",
    "try:\n",
    "  jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password).load()\n",
    "  \n",
    "  jdbcDF.show(40)\n",
    "  print(jdbcDF.count())\n",
    "except ValueError as error :\n",
    "    print(\"Connector write failed\", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_and_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
